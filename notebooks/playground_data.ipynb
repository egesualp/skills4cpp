{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeed5bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esualp/miniconda3/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Welcome to the Data Exploration Guide!\n",
      "📁 Current separator token: '<SEP>'\n",
      "💡 This notebook will help you understand the data structure step by step\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 🎯 DATA EXPLORATION NOTEBOOK - Skills4Cpp\n",
    "# ================================================\n",
    "# This notebook helps you understand the data structure and functions\n",
    "# without loading the full (huge) datasets\n",
    "\n",
    "from data import Data\n",
    "from utils import SEP_TOKEN\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "print(\"🚀 Welcome to the Data Exploration Guide!\")\n",
    "print(f\"📁 Current separator token: '{SEP_TOKEN}'\")\n",
    "print(\"💡 This notebook will help you understand the data structure step by step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 📊 1. UNDERSTANDING THE DATA STRUCTURE\n",
    "# ================================================\n",
    "print(\"📊 1. UNDERSTANDING THE DATA STRUCTURE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Let's look at what a typical data pair looks likeA\n",
    "sample_doc1 = \"role: Software Engineer \\n description: Develop software<SEP>role: Data Scientist \\n description: Analyze data\"\n",
    "sample_doc2 = \"esco role: Software Developer \\n description: Creates software applications\"\n",
    "\n",
    "print(\"🔍 A typical data pair:\")\n",
    "print(f\"📄 Document 1 (career history): {sample_doc1}\")\n",
    "print(f\"📄 Document 2 (target ESCO occupation): {sample_doc2}\")\n",
    "print()\n",
    "\n",
    "print(\"🔍 Breaking it down:\")\n",
    "print(\"   • Document 1: Your career journey (multiple jobs separated by <SEP>)\")\n",
    "print(\"   • Document 2: The target occupation you might transition to\")\n",
    "print(\"   • Goal: Learn patterns like 'Software Engineer → Data Scientist → Software Developer'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 🎭 2. EXPLORING UTILITY FUNCTIONS\n",
    "# ================================================\n",
    "print(\"🎭 2. EXPLORING UTILITY FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create some sample data to demonstrate functions|\n",
    "sample_pairs = [\n",
    "    (\"role: Software Engineer \\n description: Develop software<SEP>role: Data Scientist \\n description: Analyze data\", \"esco role: Software Developer \\n description: Creates software\"),\n",
    "    (\"role: Teacher \\n description: Teach students<SEP>role: Professor \\n description: Research and teach\", \"esco role: Lecturer \\n description: Delivers lectures\"),\n",
    "    (\"role: Nurse \\n description: Patient care\", \"esco role: Healthcare Assistant \\n description: Provides care\")\n",
    "]\n",
    "\n",
    "print(\"🔍 Sample dataset with 3 career examples:\")\n",
    "for i, (doc1, doc2) in enumerate(sample_pairs, 1):\n",
    "    print(f\"\\n{i}. Career Path {i}:\")\n",
    "    print(f\"   Input:  {doc1}\")\n",
    "    print(f\"   Target: {doc2}\")\n",
    "    print()\n",
    "\n",
    "# Demonstrate title extraction\n",
    "print(\"📝 DEMONSTRATING TITLE EXTRACTION:\")\n",
    "print(\"The _extract_titles function pulls out just the job titles:\")\n",
    "for i, (doc1, doc2) in enumerate(sample_pairs, 1):\n",
    "    roles = re.findall(r\"role: (.*?)\\n\", doc1)\n",
    "    esco_role = re.findall(r\"esco role: (.*?)\\n\", doc2)[0]\n",
    "    print(f\"{i}. {' → '.join(roles)} → {esco_role}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 🎯 3. UNDERSTANDING THE MINUS_LAST FUNCTION\n",
    "# ================================================\n",
    "print(\"🎯 3. UNDERSTANDING THE MINUS_LAST FUNCTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"🔍 What does 'minus_last' do?\")\n",
    "print(\"   • Takes your career history and removes the LAST job\")\n",
    "print(\"   • Purpose: Create training examples for 'career transition prediction'\")\n",
    "print(\"   • Example: 'Job1 → Job2 → Job3' becomes 'Job1 → Job2' (predict Job3)\")\n",
    "print()\n",
    "\n",
    "# Demonstrate with our sample data\n",
    "sample_doc1 = \"role: Software Engineer \\n description: Develop software<SEP>role: Data Scientist \\n description: Analyze data<SEP>role: ML Engineer \\n description: Build models\"\n",
    "sample_doc2 = \"esco role: AI Engineer \\n description: Develops AI systems\"\n",
    "\n",
    "print(f\"📄 Original career path: {sample_doc1}\")\n",
    "print(f\"🎯 Target prediction: {sample_doc2}\")\n",
    "print()\n",
    "\n",
    "# Show what minus_last does\n",
    "segments = sample_doc1.split(SEP_TOKEN)\n",
    "print(\"🔍 Breaking into segments:\")\n",
    "for i, segment in enumerate(segments, 1):\n",
    "    print(f\"   {i}. {segment}\")\n",
    "\n",
    "print(f\"\\n✂️  After minus_last: {SEP_TOKEN.join(segments[:-1])}\")\n",
    "print(\"🎯 Still predict: ML Engineer\")\n",
    "print(\"💡 Idea: 'Learn from incomplete career paths to predict the next step'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 🗂️  4. UNDERSTANDING DATASET TYPES\n",
    "# ================================================\n",
    "print(\"🗂️  4. UNDERSTANDING DATASET TYPES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"🔍 Available Dataset Types:\")\n",
    "print(\"   1. 'decorte' - Anonymous working histories with ESCO annotations\")\n",
    "print(\"   2. 'decorte_esco' - Same as decorte but all experiences in ESCO format\")\n",
    "print(\"   3. 'karrierewege' - Career paths from German job market\")\n",
    "print(\"   4. 'karrierewege_occ' - Karrierewege with occupational info\")\n",
    "print(\"   5. 'karrierewege_100k' - Larger Karrierewege dataset\")\n",
    "print(\"   6. 'karrierewege_cp' - Karrierewege with career path data\")\n",
    "print()\n",
    "\n",
    "print(\"🔍 Dataset Characteristics:\")\n",
    "print(\"   • decorte: Real career histories with standardized ESCO occupations\")\n",
    "print(\"   • karrierewege: German career trajectories with language variants\")\n",
    "print(\"   • _100k: Larger dataset (100k+ examples)\")\n",
    "print(\"   • _cp: Career path specific processing\")\n",
    "print(\"   • _free: Uses free text instead of ESCO format\")\n",
    "print()\n",
    "\n",
    "print(\"💡 Key Differences:\")\n",
    "print(\"   • ESCO format: Standardized job titles with descriptions\")\n",
    "print(\"   • Free text: Raw job titles and descriptions\")\n",
    "print(\"   • Size: Some datasets have 100k+ examples (hence the loading time!)\")\n",
    "print()\n",
    "\n",
    "print(\"🧠 Memory Strategy:\")\n",
    "print(\"   • Load only what you need: Use smaller datasets for exploration\")\n",
    "print(\"   • Work with subsets: Take first N examples for testing\")\n",
    "print(\"   • Use language variants: 'en_free' datasets are often smaller\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 🧪 5. WORKING WITH SMALL SUBSETS (Smart Testing)\n",
    "# ================================================\n",
    "print(\"🧪 5. WORKING WITH SMALL SUBSETS (Smart Testing)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"🔍 Why work with subsets?\")\n",
    "print(\"   • Full datasets are HUGE (100k+ examples)\")\n",
    "print(\"   • Takes minutes to load, uses lots of memory\")\n",
    "print(\"   • For development/testing: Small samples are perfect!\")\n",
    "print()\n",
    "\n",
    "print(\"💡 Strategy 1: Create your own small dataset\")\n",
    "# Create a tiny dataset for testing\n",
    "tiny_pairs = [\n",
    "    (\"role: Junior Developer \\n description: Entry level coding\", \"esco role: Software Developer \\n description: Creates software\"),\n",
    "    (\"role: Data Analyst \\n description: Work with data\", \"esco role: Data Scientist \\n description: Advanced data analysis\"),\n",
    "    (\"role: Teacher \\n description: Teach students<SEP>role: Principal \\n description: School management\", \"esco role: Education Manager \\n description: Manages educational programs\")\n",
    "]\n",
    "\n",
    "print(\"📊 Our tiny test dataset:\")\n",
    "for i, (career, target) in enumerate(tiny_pairs, 1):\n",
    "    print(f\"{i}. {career}\")\n",
    "    print(f\"   → {target}\")\n",
    "    print()\n",
    "\n",
    "print(\"💡 Strategy 2: Load just a few examples\")\n",
    "print(\"   • Stop loading after N examples\")\n",
    "print(\"   • Use sampling techniques\")\n",
    "print(\"   • Perfect for understanding structure without waiting!\")\n",
    "print()\n",
    "\n",
    "print(\"🔍 Benefits of small datasets:\")\n",
    "print(\"   • Fast loading (seconds vs minutes)\")\n",
    "print(\"   • Easy to inspect and understand\")\n",
    "print(\"   • Perfect for debugging functions\")\n",
    "print(\"   • Can manually verify correctness\")\n",
    "print()\n",
    "\n",
    "print(\"🧠 Pro Tip:\")\n",
    "print(\"   • Start with 5-10 examples to understand patterns\")\n",
    "print(\"   • Test your functions on this small set first\")\n",
    "print(\"   • Then gradually increase size for performance testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 🎪 6. UNDERSTANDING STAGES & DATA PROCESSING\n",
    "# ================================================\n",
    "print(\"🎪 6. UNDERSTANDING STAGES & DATA PROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"🔍 Available stages in get_data():\")\n",
    "print(\"   1. 'embedding_finetuning' - Full career paths for representation learning\")\n",
    "print(\"   2. 'transformation_finetuning' - Career paths with last job removed\")\n",
    "print(\"   3. 'evaluation' - Same as transformation_finetuning\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 What each stage does:\")\n",
    "print(\"   • embedding_finetuning: Learn general career patterns\")\n",
    "print(\"     Input:  'Job1 → Job2 → Job3' → Target: 'Job4'\")\n",
    "print(\"     Use:   Training embeddings, learning career trajectories\")\n",
    "print()\n",
    "print(\"   • transformation_finetuning: Learn transition patterns\")\n",
    "print(\"     Input:  'Job1 → Job2' → Target: 'Job3'\")\n",
    "print(\"     Use:   Predicting next career steps, career counseling\")\n",
    "print()\n",
    "\n",
    "print(\"🔍 ONLY_TITLES parameter:\")\n",
    "print(\"   • ONLY_TITLES=False: Full descriptions (rich information)\")\n",
    "print(\"   • ONLY_TITLES=True: Just job titles (simpler, faster)\")\n",
    "print(\"   • Choose based on your model needs!\")\n",
    "print()\n",
    "\n",
    "print(\"💡 Testing stages with our tiny dataset:\")\n",
    "# Simulate what different stages would do\n",
    "career_path = \"role: Developer \\n description: Code<SEP>role: Senior Dev \\n description: Lead teams<SEP>role: Architect \\n description: Design systems\"\n",
    "\n",
    "print(f\"Original: {career_path}\")\n",
    "print(\"Target:   esco role: Tech Lead \\n description: Leads technical teams\")\n",
    "\n",
    "# Simulate different stages\n",
    "segments = career_path.split(SEP_TOKEN)\n",
    "print(f\"\\n📊 embedding_finetuning: {SEP_TOKEN.join(segments)}\")\n",
    "print(f\"🎯 transformation_finetuning: {SEP_TOKEN.join(segments[:-1])}\")\n",
    "print(\"💡 Both predict: Architect\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 🚀 7. PRACTICAL NEXT STEPS\n",
    "# ================================================\n",
    "print(\"🚀 7. PRACTICAL NEXT STEPS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"💡 Now that you understand the structure, here's how to proceed:\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 Step 1: Start with a small, manageable dataset\")\n",
    "print(\"   data = Data('karrierewege_occ', ONLY_TITLES=True)  # Often smaller\")\n",
    "print(\"   # or\")\n",
    "print(\"   data = Data('decorte', ONLY_TITLES=True)  # Good starting point\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 Step 2: Get a subset of data for exploration\")\n",
    "print(\"   train, val, test = data.get_data('embedding_finetuning')\")\n",
    "print(\"   # Work with first 100 examples\")\n",
    "print(\"   small_train = train[:100]\")\n",
    "print(\"   small_val = val[:10]\")\n",
    "print(\"   small_test = test[:10]\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 Step 3: Explore the data structure\")\n",
    "print(\"   print('First training example:')\")\n",
    "print(\"   print(small_train[0])\")\n",
    "print(\"   print('\\\\nLabels in dataset:', len(data.labels))\")\n",
    "print(\"   print('First 5 labels:', data.labels[:5])\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 Step 4: Test different configurations\")\n",
    "print(\"   # Try different stages\")\n",
    "print(\"   train_trans, val_trans, test_trans = data.get_data('transformation_finetuning')\")\n",
    "print(\"   \")\n",
    "print(\"   # Try with full descriptions\")\n",
    "print(\"   data_full = Data('decorte', ONLY_TITLES=False)\")\n",
    "print(\"   train_full, _, _ = data_full.get_data('embedding_finetuning')\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 Step 5: Scale up gradually\")\n",
    "print(\"   # Start with 100, then 1000, then full dataset\")\n",
    "print(\"   # Monitor memory usage and loading time\")\n",
    "print()\n",
    "\n",
    "print(\"🔥 Pro Tips:\")\n",
    "print(\"   • Use ONLY_TITLES=True for faster loading and simpler models\")\n",
    "print(\"   • Start with 'karrierewege_occ' - often smaller than others\")\n",
    "print(\"   • Use subsets during development, full datasets for final training\")\n",
    "print(\"   • Monitor memory: Large datasets can use 10GB+ RAM\")\n",
    "print()\n",
    "\n",
    "print(\"🎉 You're ready to explore! The key is starting small and scaling up.\")\n",
    "print(\"💡 Remember: Understanding 100 examples well is better than loading 100k poorly!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🏆 CONGRATULATIONS! You now understand:\")\n",
    "print(\"   ✅ Data structure (career paths → target occupations)\")\n",
    "print(\"   ✅ Utility functions (minus_last, extract_titles)\")\n",
    "print(\"   ✅ Dataset types and their differences\")\n",
    "print(\"   ✅ Processing stages and their purposes\")\n",
    "print(\"   ✅ Smart strategies for working with large datasets\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 🧪 PRACTICAL EXAMPLE: Working with Small Data\n",
    "# ================================================\n",
    "print(\"🧪 PRACTICAL EXAMPLE: Working with Small Data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a mock version of the loading functions to avoid the huge datasets\n",
    "def create_small_test_dataset():\n",
    "    \"\"\"Create a small dataset for testing - simulates what the real functions do\"\"\"\n",
    "    return [\n",
    "        # (career_history, target_occupation)\n",
    "        (\"role: Junior Developer \\n description: Entry level coding<SEP>role: Software Engineer \\n description: Full stack development\",\n",
    "         \"esco role: Senior Developer \\n description: Leads development projects\"),\n",
    "\n",
    "        (\"role: Data Analyst \\n description: Basic data analysis<SEP>role: Business Analyst \\n description: Business insights\",\n",
    "         \"esco role: Data Scientist \\n description: Advanced analytics and modeling\"),\n",
    "\n",
    "        (\"role: Teacher \\n description: Elementary education<SEP>role: Department Head \\n description: Curriculum management\",\n",
    "         \"esco role: Education Manager \\n description: Manages educational programs\"),\n",
    "\n",
    "        (\"role: Nurse \\n description: Patient care<SEP>role: Senior Nurse \\n description: Team coordination\",\n",
    "         \"esco role: Nursing Manager \\n description: Manages nursing staff\"),\n",
    "\n",
    "        (\"role: Sales Rep \\n description: Direct sales<SEP>role: Sales Manager \\n description: Team leadership\",\n",
    "         \"esco role: Sales Director \\n description: Strategic sales planning\")\n",
    "    ]\n",
    "\n",
    "# Simulate the Data class functionality\n",
    "class MockData:\n",
    "    def __init__(self, data_type, ONLY_TITLES=False):\n",
    "        self.data_type = data_type\n",
    "        self.only_titles = ONLY_TITLES\n",
    "        self.train_pairs = create_small_test_dataset()\n",
    "        self.val_pairs = create_small_test_dataset()[:2]  # Just 2 for validation\n",
    "        self.test_pairs = create_small_test_dataset()[:1]  # Just 1 for testing\n",
    "        self.labels = list(set([pair[1] for pair in self.train_pairs + self.val_pairs + self.test_pairs]))\n",
    "\n",
    "    def get_data(self, stage):\n",
    "        if stage == 'embedding_finetuning':\n",
    "            if self.only_titles:\n",
    "                return self._extract_titles(self.train_pairs), self._extract_titles(self.val_pairs), self._extract_titles(self.test_pairs)\n",
    "            else:\n",
    "                return self.train_pairs, self.val_pairs, self.test_pairs\n",
    "        elif stage in ['transformation_finetuning', 'evaluation']:\n",
    "            if self.only_titles:\n",
    "                return self._minus_last(self._extract_titles(self.train_pairs)), self._minus_last(self._extract_titles(self.val_pairs)), self._minus_last(self._extract_titles(self.test_pairs))\n",
    "            else:\n",
    "                return self._minus_last(self.train_pairs), self._minus_last(self.val_pairs), self._minus_last(self.test_pairs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _minus_last(data_pairs):\n",
    "        \"\"\"Remove last segment from career histories\"\"\"\n",
    "        result = []\n",
    "        for doc1, doc2 in data_pairs:\n",
    "            segments = doc1.split(SEP_TOKEN)\n",
    "            if len(segments) > 1:\n",
    "                new_doc1 = SEP_TOKEN.join(segments[:-1])\n",
    "                result.append((new_doc1, doc2))\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_titles(data_pairs):\n",
    "        \"\"\"Extract just the job titles\"\"\"\n",
    "        result = []\n",
    "        for doc1, doc2 in data_pairs:\n",
    "            roles = re.findall(r\"role: (.*?)\\n\", doc1)\n",
    "            esco_role = re.findall(r\"esco role: (.*?)\\n\", doc2)[0]\n",
    "            result.append((SEP_TOKEN.join(roles), esco_role))\n",
    "        return result\n",
    "\n",
    "# Test it!\n",
    "print(\"🔍 Creating a small mock dataset (5 examples)...\")\n",
    "mock_data = MockData('decorte', ONLY_TITLES=False)\n",
    "\n",
    "print(f\"✅ Created dataset with {len(mock_data.train_pairs)} training examples\")\n",
    "print(f\"📊 Validation set: {len(mock_data.val_pairs)} examples\")\n",
    "print(f\"🧪 Test set: {len(mock_data.test_pairs)} examples\")\n",
    "print(f\"🏷️  Unique labels: {len(mock_data.labels)}\")\n",
    "print()\n",
    "\n",
    "print(\"🔍 Let's look at the first example:\")\n",
    "train, val, test = mock_data.get_data('embedding_finetuning')\n",
    "print(f\"Training example 1: {train[0]}\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 Now try with ONLY_TITLES=True:\")\n",
    "mock_data_titles = MockData('decorte', ONLY_TITLES=True)\n",
    "train_titles, val_titles, test_titles = mock_data_titles.get_data('embedding_finetuning')\n",
    "print(f\"Title-only example: {train_titles[0]}\")\n",
    "print()\n",
    "\n",
    "print(\"✂️  Try transformation_finetuning (removes last job):\")\n",
    "train_trans, _, _ = mock_data.get_data('transformation_finetuning')\n",
    "print(f\"After minus_last: {train_trans[0]}\")\n",
    "print(\"💡 Notice: 'Software Engineer' was removed, still predict 'Senior Developer'\")\n",
    "\n",
    "print(\"\\n🎉 Success! Now you can experiment without loading huge datasets!\")\n",
    "print(\"💡 Use this approach for quick testing and understanding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5348fe61",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkarrierewege+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train, val, test \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_finetuning\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(train)\n",
      "File \u001b[0;32m~/repos/skills4cpp/src/data.py:45\u001b[0m, in \u001b[0;36mData.__init__\u001b[0;34m(self, DATA_TYPE, DOC_1_PROMPT, DOC_2_PROMPT, ONLY_TITLES, max_rows)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/skills4cpp/src/data.py:80\u001b[0m, in \u001b[0;36mData.__load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pairs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_pairs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_pairs \u001b[38;5;241m=\u001b[39m load_prepare_karrierewege(\n\u001b[1;32m     76\u001b[0m         consider_all_subspans_of_len_at_least_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, minus_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_free_cp\u001b[39m\u001b[38;5;124m'\u001b[39m, max_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Extract unique labels from the dataset\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m([pair[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_pairs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_pairs\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_pairs]))\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "data = Data('karrierewege+', max_rows = 1000)\n",
    "train, val, test = data.get_data('embedding_finetuning')\n",
    "df_train = pd.DataFrame(train)\n",
    "df_train.iloc[1]\n",
    "df_train.info()\n",
    "print(\"\\n\".join(df_train.iloc[1].to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17868e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esualp/miniconda3/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/occupations_en.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Tuple\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# import DECORTE\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# The DATA_TYPE is case-sensitive, so it should be 'decorte'\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m decorte_data \u001b[38;5;241m=\u001b[39m \u001b[43mData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecorte\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m train_decorte, val_decorte, test_decorte \u001b[38;5;241m=\u001b[39m decorte_data\u001b[38;5;241m.\u001b[39mget_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_finetuning\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m df_decorte_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(train_decorte)\n",
      "File \u001b[0;32m~/repos/skills4cpp/src/data.py:45\u001b[0m, in \u001b[0;36mData.__init__\u001b[0;34m(self, DATA_TYPE, DOC_1_PROMPT, DOC_2_PROMPT, ONLY_TITLES, max_rows)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/skills4cpp/src/data.py:55\u001b[0m, in \u001b[0;36mData.__load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mLoads data based on the specified `DATA_TYPE`.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mDepending on the dataset type, this method calls the appropriate `load_prepare_*` function \u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03mto load and preprocess the dataset. It also extracts unique labels from the dataset.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDATA_TYPE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecorte\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pairs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_pairs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mload_prepare_decorte\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsider_all_subspans_of_len_at_least_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminus_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_rows\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDATA_TYPE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecorte_esco\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pairs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_pairs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_pairs \u001b[38;5;241m=\u001b[39m load_prepare_decorte_esco(\n\u001b[1;32m     60\u001b[0m         consider_all_subspans_of_len_at_least_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, minus_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, max_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rows\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/skills4cpp/src/utils.py:230\u001b[0m, in \u001b[0;36mload_prepare_decorte\u001b[0;34m(minus_last, consider_all_subspans_of_len_at_least_2, verbose, max_len, max_rows)\u001b[0m\n\u001b[1;32m    227\u001b[0m     dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m example: replace_esco_titles(example, i))\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Load descriptions for ESCO occupations\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m ESCO_occupations \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moccupations_en.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Create dictionary for ESCO occupations\u001b[39;00m\n\u001b[1;32m    234\u001b[0m ESCO_occupations_dict \u001b[38;5;241m=\u001b[39m ESCO_occupations\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconceptUri\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m ]\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/occupations_en.csv'"
     ]
    }
   ],
   "source": [
    "from data import Data\n",
    "from utils import SEP_TOKEN\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "# import DECORTE\n",
    "# The DATA_TYPE is case-sensitive, so it should be 'decorte'\n",
    "decorte_data = Data('decorte', max_rows=100)\n",
    "train_decorte, val_decorte, test_decorte = decorte_data.get_data('embedding_finetuning')\n",
    "df_decorte_train = pd.DataFrame(train_decorte)\n",
    "df_decorte_train.info()\n",
    "print(df_decorte_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a767455",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
